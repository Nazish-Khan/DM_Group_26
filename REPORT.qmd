---
title: "E-commerce Database Design"
author: "IB9HP0 Group_26"
format: pdf 
code-overflow: wrap
geometry: 
      - top=30mm
      - left=20mm
      - right=20mm
      - heightrounded
margin:
      x: 2.53cm
      y: 2.53cm
editor: visual
fontfamily: libertinus
toc: true
---

{{< pagebreak >}}

## Introduction

The introduction of E-commerce has revolutionised the way businesses operate. It allows customers to shop from a global range of products from the convenience of their homes. Most organisations use database structures to manage the functioning of these platforms, as they allow for the control of large amounts of information. A well-designed e-commerce database serves as a foundation for the platform's effective operations.

In this report, we will delve into the structure of our e-commerce database by providing an overview of the model. Starting with the ER diagram and schema creation, after which we generate the synthetic data and create a GitHub repository. This is followed by the visualisation of our data generated.

The workflows associated with our database can be found in the GitHub link provided here - <https://github.com/Nazish-Khan/DM_Group_26>

{{< pagebreak >}}

# Part 1: Database Design process

### 1.1.1 Conceptual Schema

Following a process, The ER serves as a visual representation of the structure of our database our system. Its purpose is to provide a clear depiction of the entities within the database and the relationships that exist between them. By examining this diagram, stakeholders gain insights into the organisation and interconnections of data within the database.

**Entities**

1.  SUPPLIERS - Represent entities that supply products into our system.

2.  ADVERTISEMENTS - Represent promotional advertisements associated with products and the webpage.

3.  ORDER DETAILS - Represent the details of multiple orders placed by individual customers.

4.  PRODUCTS - Represent individual products available in our system.

5.  CATEGORY - Represents categories or types to which products belong.

6.  CUSTOMERS - Represent individual customers who place orders.

7.  WAREHOUSES - Represents physical locations where products are stored.

Figure 1 below illustrates our Entity-Relationship (E-R) diagram, showcasing seven entities along with their respective attributes. Within the diagram, we observe several one-to-many relationships and many-to-many relationships existing between entities as well as self-referencing within a customer entity. Through this diagram, we can visualise how different entities interact and relate to each other within the database.

![Figure 1 - ER Diagram](project_pics/ER_Diagram.png){width="60%"}

{{< pagebreak >}}

### 1.1.2 Relationship Sets and Cardinality

From the ER diagram, we have formed our relationship and in the image below we have illustrated 6 different relationship sets with different carnality.

![Figure 2- Relationship Sets](project_pics/relationship_sets.png){width="80%"}

1.  **Advertisement-display-Products**: In this [many-to-one relationship]{.underline} set, we see multiple advertisements being displayed for a single product. This relationship enables advertisers to create different advertisements to promote the same product. Furthermore, target different audiences and highlight different aspects of the product's features and benefits.

2.  **Warehouse-store-Products:** This is a [many-to-many relationship]{.underline} set, meaning that both multiple products can be stored in multiple warehouses. This structure makes for highly flexible inventory management, allowing for distribution of products across different warehouses. As a result, this relationship allows efficient inventory management by allocating stock based on demand and location.

3.  **Supplier-purchase-Advertisement**: In this [one-to-many relationship]{.underline} set, multiple advertisements can be purchased by a single supplier. This means that different advertisements promoting various products could be purchased from the same supplier. Therefore, the model can effectively showcase the supplier's range of products.

4.  **Supplier-supply-Product**: In this [many-to-many relationship]{.underline}, a supplier can provide various products to an e-commerce platform, while each product can be sourced from multiple suppliers. This relationship provides flexibility in managing inventory and sourcing products.

5.  **Products-include-Category**: This [many-to-one relationship]{.underline} allows for efficient organisation and classification of products within the e-commerce platform. It enables customers to browse and search for products based on specific categories, making it easier to navigate and find relevant items.

6.  **Product-Order details â€“ Supplier- Customer- order**: In an e-commerce platform, the [many-to-many relationship]{.underline} among Products, Orders, Customers, Suppliers, and Order Details streamlines complex interactions. It enables multiple products to be associated with multiple orders, facilitating flexible inventory management and order fulfillment. This relationship structure is crucial for effective inventory, order, and supplier management, ensuring seamless operations in the e-commerce environment.

{{< pagebreak >}}

### 1.1.3 Assumptions

-   Assumed no multi-value variables exist - for example, all customers have singular addresses and phone numbers.

-   Each product assumed to be assigned only one category - no multi-category products exist.

-   The eCommerce system is assumed to be a single database representing the structure of a single database system - no interactions from external systems or databases unless they are integral to the design.

-   Security measures such as user authentication, data encryption and compliance with regulatory bodies (GDPR, consumer act and etc) have been met.

-   Assumed that the system handles payment processing securely with the integration of gateways to facilitate multiple payment methods (such as credit/debit cards, digital wallets, etc.)

-   Assumed customers can refer other customers with a main customer base from Western Europe

-   The store date is included to identify the product quantity kept at each warehouse on the particular day for the purpose of an inventory management.

-   Due to geographical considerations and operational efficiency, we will not factor in discounts and supplier bulk-buying.

-   All transactions carried out using Euro.

### 1.1.4 Logical Schema

After transferring the relational schema to logical, we have the following tables (underline denote primary key, double underline denote foreign key) with the addition of each table attributes.

For Entity tables

-   **SUPPLIERS** ($\underline{supplier\_id}$, supplier_name, supplier_address, supplier_contact, supplier_email)

-   **CUSTOMERS** ($\underline{customer\_id}$, customer_name, customer_address, customer_email, customer_contact, $\underline{\underline{related\_id}}$)

-   **ADVERTISEMENTS** ($\underline{ads\_id}$, ads_startdate, ads_enddate, ads_price, $\underline{\underline{supplier\_id}}$, $\underline{\underline{product\_id}}$)

-   **WAREHOUSES** ($\underline{warehouse\_id}$, warehouse_address, warehouse_contact)

-   **PRODUCTS** ($\underline{product\_id}$, product_name, product_reviewscore, product_price, $\underline{\underline{category\_id}}$)

-   **CATEGORY** ($\underline{category\_id}$, category_name)

-   **ORDER_DETAILS** ($\underline{order\_id}$, order_status, check_outdate, payment_method, payment_status, payment_date)

For relationship table

-   **SUPPLY** ($\underline{\underline{product\_id},\underline{supplier\_id}}$, product_cost, supply_quantity)

-   **STORE** ($\underline{\underline{product\_id},\underline{warehouse\_id},store\_date}$, store_quantity)

-   **ORDER** ($\underline{\underline{product\_id},\underline{customer\_id},\underline{supplier\_id},\underline{order\_id}}$, order_transaction_date, order_quantity)

## 1.2.1 Normalisation

However, we noticed that the address attributes in SUPPLIERS, CUSTOMERS and WAREHOUSES table were not normalised up to 3NF as the address details (contained information of street, city, country and postcode) which were non-atomic value.

Therefore, we resolved this problem by splitting up the address column of address in these table to be street, city, country and postcode. The logical schema of these three tables after normalisation process are as follow;

-   **SUPPLIERS** ($\underline{supplier\_id}$, supplier_name, supplier_street, supplier_city, supplier_country, supplier_postcode, supplier_contact, supplier_email)

-   **CUSTOMERS** ($\underline{customer\_id}$, customer_name, customer_street, customer_city, customer_country, customer_postcode, customer_email, customer_contact, $\underline{\underline{related\_id}}$)

-   **WAREHOUSES** ($\underline{warehouse\_id}$, warehouse_street, warehouse_city, warehouse_country, warehouse_postcode, warehouse_contact)

-   After we ensured that our logical schema was designed up to 3NF, we started the process of transforming it to a physical schema by creating its skeleton using SQLite.

We determine the type of each attributes as follows;

| Attributes                                   | Type         | Reason                                                                                                                                                                                                                                      |
|-------------------|-------------------|-----------------------------------|
| ID                                           | VARCHAR(15)  | We decide to synthesis our ID data by using a combination of uppercase letter(s) with digits. We select this pattern to ensure that the data contain in this format is unique since we will use these item as a primary key in many tables. |
| Name, Street, City, Country, Postcode, Email | VARCHAR(255) | We select the type VARCHAR for these attributes since these items are information about location or information that can be read and understand by the users who have an access to this database.                                           |
| Date                                         | DATE         | We select the DATE type in respond to the value we will contain under this attribute.                                                                                                                                                       |
| Price, Quantity                              | INT          | As these items must be able to compute and the nature of these information represent a numerical value.                                                                                                                                     |

## 1.2.2 Physical Schema

After considering the circumstances above, physical schema using SQLite was created as follows;

```{r creationTable .code-overflow-wrap}

# Read the relevant libraries
library(RSQLite)
library(readr)

# Specify the database file name
db_file <- "database/database.db"

# Create the db connection with SQLite
my_connection <- RSQLite::dbConnect(RSQLite::SQLite(),dbname = db_file)
print(my_connection)

# Delete if tables already exist
#drop_tables <- "DROP TABLE IF EXISTS SUPPLIERS, CUSTOMERS, 
#PRODUCTS, ADS, WAREHOUSE, CATEGORY,ORDER_DETAILS, ORDERS_PAYMENT, SUPPLY, STORE, ORDERS;"

# Entity Table Creation

# Create table for SUPPLIERS entity
sql_suppliers <- "
CREATE TABLE IF NOT EXISTS SUPPLIERS (
    supplier_id VARCHAR(15) PRIMARY KEY,
    supplier_name VARCHAR(255),
    supplier_street VARCHAR(255),
    supplier_city VARCHAR(255),
    supplier_country VARCHAR(255),
    supplier_postcode VARCHAR(255),
    supplier_contact INT,
    supplier_email VARCHAR(255)
);"

# Create table for Customer entity
sql_customers <- "
CREATE TABLE IF NOT EXISTS CUSTOMERS (
    customer_id VARCHAR(15) PRIMARY KEY,
    customer_name VARCHAR(255),
    customer_street VARCHAR(255),
    customer_city VARCHAR(255),
    customer_country VARCHAR(255),
    customer_postcode VARCHAR(255),
    customer_email VARCHAR(255),
    customer_contact INT,
    related_id VARCHAR(15),
    FOREIGN KEY (related_id) REFERENCES CUSTOMERS(customer_id)
);"

# create table for Advertisements entity
sql_ads <- "
CREATE TABLE IF NOT EXISTS ADVERTISEMENTS (
    ads_id VARCHAR(15) PRIMARY KEY,
    ads_startdate DATE,
    ads_enddate DATE,
    ads_price INT,
    supplier_id VARCHAR(15),
    product_id VARCHAR(15),
    FOREIGN KEY (supplier_id) REFERENCES SUPPLIERS(supplier_id),
    FOREIGN KEY (product_id) REFERENCES PRODUCTS(product_id)
);"

# create table for WAREHOUSES entity
sql_warehouses <- "
CREATE TABLE IF NOT EXISTS WAREHOUSES (
    warehouse_id VARCHAR(15) PRIMARY KEY,
    warehouse_street VARCHAR(255),
    warehouse_city VARCHAR(255),
    warehouse_country VARCHAR(255),
    warehouse_postcode VARCHAR(255),
    warehouse_contact VARCHAR(255)
);"


# create table for PRODUCTS entity
sql_products <- "
CREATE TABLE IF NOT EXISTS PRODUCTS (
    product_id VARCHAR(15) PRIMARY KEY,
    product_name VARCHAR(255),
    product_reviewscore INT,
    product_price INT,
    category_id VARCHAR(15),
    FOREIGN KEY (category_id) REFERENCES CATEGORY(category_id)
);"


# create table for CATEGORY entity
sql_category <- "
CREATE TABLE IF NOT EXISTS CATEGORY (
    category_id VARCHAR(15) PRIMARY KEY,
    category_name VARCHAR(255)
);"


# create table for ORDERS_DETAILS entity
sql_order_details <- "
CREATE TABLE IF NOT EXISTS ORDERS_DETAILS (
    order_id VARCHAR(15) PRIMARY KEY,
    order_status VARCHAR(255),
    check_out_date DATE,
    payment_method VARCHAR(255),
    payment_status VARCHAR(255),
    payment_date DATE
);"

# Relationship Entities

# create table for SUPPLY relationship entity
sql_supply <- "
CREATE TABLE IF NOT EXISTS SUPPLY (
    product_id VARCHAR(15),
    supplier_id VARCHAR(15),
    product_cost INT,
    supply_quantity INT,
    PRIMARY KEY (product_id, supplier_id),
    FOREIGN KEY (product_id) REFERENCES PRODUCTS(product_id),
    FOREIGN KEY (supplier_id) REFERENCES SUPPLIERS(supplier_id)
);"

# create table for STORE relationship
sql_store <- "
CREATE TABLE IF NOT EXISTS STORE (
    product_id VARCHAR(15),
    warehouse_id VARCHAR(15),
    store_date DATE,
    store_quantity INT,
    PRIMARY KEY (product_id, warehouse_id, store_date),
    FOREIGN KEY (product_id) REFERENCES PRODUCTS(product_id),
    FOREIGN KEY (warehouse_id) REFERENCES WAREHOUSES(warehouse_id)
);"

# create table for ORDERS relationship
sql_orders <- "
CREATE TABLE IF NOT EXISTS ORDERS (
    product_id VARCHAR(15),
    customer_id VARCHAR(15),
    supplier_id VARCHAR(15),
    order_id VARCHAR(15),
    order_transaction_date DATE,
    order_quantity INT,
    PRIMARY KEY (product_id, customer_id, supplier_id,  order_id),
    FOREIGN KEY (product_id) REFERENCES PRODUCTS(product_id),
    FOREIGN KEY (customer_id) REFERENCES CUSTOMERS(customer_id),
    FOREIGN KEY (supplier_id) REFERENCES SUPPLIERS(supplier_id),
    FOREIGN KEY (order_id) REFERENCES ORDERS_DETAILS(order_id)
);"

```

```{r Execute, eval= FALSE}

# Execute the SQL statements
result <- RSQLite::dbExecute(my_connection, sql_suppliers)
print(result)
RSQLite::dbExecute(my_connection, sql_customers)
RSQLite::dbExecute(my_connection, sql_category)
RSQLite::dbExecute(my_connection, sql_products)
RSQLite::dbExecute(my_connection, sql_ads)
RSQLite::dbExecute(my_connection, sql_warehouses)
RSQLite::dbExecute(my_connection, sql_order_details)
RSQLite::dbExecute(my_connection, sql_supply)
RSQLite::dbExecute(my_connection, sql_store)
RSQLite::dbExecute(my_connection, sql_orders)

```

```{r List Tables, echo=FALSE}

# List all tables in the database
tables <- dbListTables(my_connection)

# Iterate over tables and print the schema
for (table in tables) {
  cat("Table:", table, "\n")
  fields <- dbListFields(my_connection, table)
  print(fields)
  cat("\n")
}
# Close the database connection
RSQLite::dbDisconnect(my_connection)

```

{{< pagebreak >}}

# PART 2: Data Generation and Management

## 2.1 - Synthetic Data Generation

We used a generative A.I. (such as a data generating website like 'Mockaroo') to generate a data set for each entity; making sure that the synthetic data was realistic and logical. After receiving these synthetic data set, we create foreign key(s) for any required entity. We also generate relationship table in an R software to ensure that the composite keys in these entities are selected appropriately.

The code associated with foreign key relation can be found in the GitHub link provided here - <https://github.com/Nazish-Khan/DM_Group_26/blob/main/R/Data_Manipulation.Rmd>

Please see our example of generative data sets below;

Table 1.1: Supplier data set

```{r, echo=FALSE }
library(readr) 
library(kableExtra)

supplier_data <- read_csv("data_upload/Supplier/supplier_data_v1.csv",show_col_types = FALSE) 
kable(head(supplier_data, 2)) %>% kable_styling(font_size = 6)

```

Table 1.2: Customer data set

```{r, echo=FALSE}
customer_data <- read_csv("data_upload/Customer/customer_data_v1.csv",show_col_types = FALSE) 

kable(head(customer_data, 5)) %>% kable_styling(font_size = 6)
```

Table 1.3: Advertisement data set

```{r, echo=FALSE}
advertisement_data <- read_csv("data_upload/Advertisement/advertisement_data_v1.csv",show_col_types = FALSE) 

kable(head(advertisement_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.4: Warehouse data set

```{r, echo=FALSE}
warehouse_data <- read_csv("data_upload/Warehouse/warehouse_data_v1.csv",show_col_types = FALSE) 

kable(head(advertisement_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.5: Product data set

```{r, echo=FALSE}
product_data <- read_csv("data_upload/Product/product_data_v1.csv",show_col_types = FALSE) 

kable(head(product_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.6: category data set

```{r, echo=FALSE}
category_data <- read_csv("data_upload/Category/category_data_v1.csv",show_col_types = FALSE) 

kable(head(category_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.7: Order Details data set

```{r, echo=FALSE}
order_details_data <- read_csv("data_upload/Order_details/order_details_data_v1.csv",show_col_types = FALSE) 

kable(head(order_details_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.8: Supply data set

```{r, echo=FALSE}
supply_data <- read_csv("data_upload/Supply/supply_data_v1.csv",show_col_types = FALSE) 
kable(head(supply_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.9: Store data set

```{r, echo=FALSE}
store_data <- read_csv("data_upload/Store/store_data_v1.csv",show_col_types = FALSE) 

kable(head(store_data, 5)) %>% kable_styling(font_size = 10)
```

Table 1.10: order data set

```{r, echo=FALSE}
order_data <- read_csv("data_upload/Order/order_data_v1.csv",show_col_types = FALSE) 

kable(head(order_data, 5)) %>% kable_styling(font_size = 10)
```

{{< pagebreak >}}

## 2.2 - Data Import and Quality Assurance

-   **Data Import:**

To streamline our data import process, we've established a structured approach. Every table has its own folder under the "data_upload" directory that contains CSV files with a version suffix (e.g., "\_data_v1.csv"). Every update to the data is posted into the folder associated with that table. We run a transformation.R script that is specifically made for this purpose in order to add the most recent version of the data to our database. This effective procedure makes sure that our database is always current with the most recent revisions of the data, which makes data administration and analysis easier.

-   **Quality Assurance:**

In order to achieve our goal of importing produced data into the SQL database while maintaining data quality, we have coded an extensive data import procedure. Quality assurance procedures and thorough data integrity checks are part of this process. To ensure adherence to common norms, we have specifically created validation methods for the following checks:

1.  Entity Integrity Constraint Check: To identify the null primary keys in the data to be loaded
2.  Email Format Check: To test the format of email addresses.
3.  Validated Primary Key Format: To check format for all primary keys of the database tables.
4.  Duplicate Records Identification: To preserve the coherence of the database, we have also incorporated methods for locating and removing duplicate records.
5.  Referential Integrity Check: In addition, our script includes strong validations to guarantee the correct establishment of foreign key associations in order to protect referential integrity.

This improves the database's consistency and dependability. With the use of these safeguards, our data import procedure not only adds data to the database but also ensures its integrity and quality, setting the stage for further operations and analysis.

```{r tranformationFile, warning=FALSE}

library(RSQLite)
library(readr)
library(stringr)
library(fs)
library(dplyr)
library(DBI)


# Function to find the latest version of a file in a folder
find_latest_version <- function(folder_path, prefix) {
  files <- list.files(folder_path, pattern = paste0("^", prefix, "_v[0-9]+\\.csv$"))
  versions <- as.numeric(sub(paste0("^", prefix, "_v([0-9]+)\\.csv$"), "\\1", files))
  latest_version <- max(versions)
  latest_file <- paste0(prefix, "_v", latest_version, ".csv")
  return(file.path(folder_path, latest_file))
}

# Data Load and Data Integrity Check

#Adding testing comments
db_file <- "database/database.db"
my_connection <- RSQLite::dbConnect(RSQLite::SQLite(),dbname = db_file)

# Specify the folder paths for each type of data
customer_folder <- "data_upload/Customer"
supplier_folder <- "data_upload/Supplier"
advertisement_folder <- "data_upload/Advertisement"
category_folder <- "data_upload/Category"
order_details_folder <- "data_upload/Order_details"
order_folder <- "data_upload/Order"
product_folder <- "data_upload/Product"
store_folder <- "data_upload/Store"
supply_folder <- "data_upload/Supply"
warehouse_folder <- "data_upload/Warehouse"

# Load product and supplier data from the latest CSV files
customer_file <- find_latest_version(customer_folder, "customer_data")
supplier_file <- find_latest_version(supplier_folder, "supplier_data")
advertisement_file <- find_latest_version(advertisement_folder, "advertisement_data")
category_file <- find_latest_version(category_folder, "category_data")
order_details_file <- find_latest_version(order_details_folder, "order_details_data")
order_file <- find_latest_version(order_folder, "order_data")
product_file <- find_latest_version(product_folder, "product_data")
store_file <- find_latest_version(store_folder, "store_data")
supply_file <- find_latest_version(supply_folder, "supply_data")
warehouse_file <- find_latest_version(warehouse_folder, "warehouse_data")

print("Reading the data")

# Load product and supplier data from CSV files
# Reading the csv file
customer_data <- readr::read_csv(customer_file,show_col_types = FALSE)
supplier_data <- readr::read_csv(supplier_file,show_col_types = FALSE)
advertisement_data <- readr::read_csv(advertisement_file,show_col_types = FALSE)
category_data <- readr::read_csv(category_file,show_col_types = FALSE)
order_data <- readr::read_csv(order_file,show_col_types = FALSE)
order_details_data <- readr::read_csv(order_details_file,show_col_types = FALSE)
product_data <- readr::read_csv(product_file,show_col_types = FALSE)
store_data <- readr::read_csv(store_file,show_col_types = FALSE)
supply_data <- readr::read_csv(supply_file,show_col_types = FALSE)
warehouse_data <- readr::read_csv(warehouse_file,show_col_types = FALSE)


# Data Integrity Check


errors <- list()
# Check for null primary keys
null_keys <- c(
  "supplier_data" = "supplier_id",
  "customer_data" = "customer_id",
  "advertisement_data" = "ads_id",
  "warehouse_data" = "warehouse_id",
  "product_data" = "product_id",
  "category_data" = "category_id",
  "order_details_data" = "order_id"
)
# 
# Check for null primary keys
for (table_name in names(null_keys)) {
  if (any(is.na(eval(parse(text = table_name))[[null_keys[table_name]]]))) {
    errors[[table_name]] <- paste("Null primary key detected in", null_keys[table_name])
  }
}

# Check for null values in composite primary key columns
composite_keys <- list(
  "supply_data" = c("product_id", "supplier_id"),
  "store_data" = c("product_id", "warehouse_id"),
  "order_data" = c("order_id", "customer_id", "product_id", "supplier_id")
)

for (table_name1 in names(composite_keys)) {
  # Get the column names for the composite primary key
  key_columns <- composite_keys[[table_name1]]
  
  # Extract the data frame corresponding to the table name
  table_data1 <- eval(parse(text = table_name1))
  #print(table_data1)
  # Check for null values in each column of the composite primary key
  for (key_column in key_columns) {
    if (any(is.na(table_data1[[key_column]]))) {
      errors[[table_name]] <- paste("Null value detected in", key_column, "of", table_name1,"\n")
      #cat("Null value detected in", key_column, "of", table_name1, "\n")
    }
  }
}
# Function to check email format
validate_email <- function(email) {
  return(str_detect(email, "^\\S+@\\S+\\.\\S+$"))
}

# Validate email format
invalid_customer_emails <- which(!validate_email(customer_data$customer_email))
if (length(invalid_customer_emails) > 0) {
  errors$customer_email <- paste("Invalid email format detected for customers:",
                                 toString(invalid_customer_emails))
}
invalid_supplier_emails <- which(!validate_email(supplier_data$supplier_email))
if (length(invalid_supplier_emails) > 0) {
  errors$supplier_email <- paste("Invalid email format detected for suppliers:",
                                 toString(invalid_supplier_emails))
}

# Validate Primary Key Format
# Check supplier_id format for all records
supplier_id_pattern <- "^SUP\\d{9,}$"     # Define regex pattern for supplier_id
invalid_supplier_ids <- which(!grepl(supplier_id_pattern, supplier_data$supplier_id))
if (length(invalid_supplier_ids) > 0) {
  errors$supplier_id <- paste("Invalid supplier IDs detected at index:", 
                              toString(invalid_supplier_ids))
}

# Advertisement ID Format
ads_id_pattern <- "^AD\\d{9,}$"                # Define regex pattern for ads_id
invalid_ads_ids <- which(!grepl(ads_id_pattern, advertisement_data$ads_id))
if (length(invalid_ads_ids) > 0) {
  errors$ads_id <- paste("Invalid Ads IDs detected at index:", 
                         toString(invalid_ads_ids))
}

# Customer ID Format
cust_id_pattern <- "^CUS\\d{9,}$"                # Define regex pattern for customer_id
invalid_cust_ids <- which(!grepl(cust_id_pattern, customer_data$customer_id))
if (length(invalid_cust_ids) > 0) {
  errors$customer_id <- paste("Invalid Customer IDs detected at index:", 
                              toString(invalid_cust_ids))
}

# Product ID Format
product_id_pattern <- "^P\\d{8,}$"                # Define regex pattern for product_id
invalid_product_ids <- which(!grepl(product_id_pattern, product_data$product_id))
if (length(invalid_product_ids) > 0) {
  errors$product_id <- paste("Invalid Product IDs detected at index:", 
                             toString(invalid_product_ids))
}

# Category ID Format
category_id_pattern <- "^CAT\\d{10}$"                # Define regex pattern for product_id
invalid_cat_ids <- which(!grepl(category_id_pattern, category_data$category_id))
if (length(invalid_cat_ids) > 0) {
  errors$category_id <- paste("Invalid Category IDs detected at index:", 
                              toString(invalid_cat_ids))
}

# Order ID Format
ord_id_pattern <- "^ORD\\d{10}$"                # Define regex pattern for order_id
invalid_ord_ids <- which(!grepl(ord_id_pattern, order_details_data$order_id))
if (length(invalid_ord_ids) > 0) {
  errors$order_id <- paste("Invalid Order IDs detected at index:", 
                           toString(invalid_ord_ids))
}

# Warehouse ID Format
ware_id_pattern <- "^W\\d{8,}$"                # Define regex pattern for Warehouse ID
invalid_ware_ids <- which(!grepl(ware_id_pattern, warehouse_data$warehouse_id))
if (length(invalid_ware_ids) > 0) {
  errors$warehouse_id <- paste("Invalid Warehouse IDs detected at index:", 
                               toString(invalid_ware_ids))
}

# Check for duplicate records in single primary key entities

for (table_name in names(null_keys)) {
  if (anyDuplicated(eval(parse(text = table_name))[[null_keys[table_name]]])) {
    errors[[table_name]] <- paste("Duplicate records detected for", null_keys[table_name])
  }
}

# Check for duplicate records in composite primary key entities
for (table_name in names(composite_keys)) {
  key_columns <- composite_keys[[table_name]]
  table_data <- eval(parse(text = table_name))
  
  if (anyDuplicated(table_data[key_columns])) {
    errors[[table_name]] <- paste("Duplicate records detected for composite primary key in",table_name)
  }
}

# Check for errors in the errors list
if (length(errors) > 0) {
  stop("Data validation failed:", errors)
} else {
  
# Writing the data to the database
  print("Writing to the database")

# Append only new records to the existing table
  print("Adding Supplier Data")
  RSQLite::dbWriteTable(my_connection, "SUPPLIERS", supplier_data, append = TRUE)
  print("Adding Category Data")
  RSQLite::dbWriteTable(my_connection, "CATEGORY", category_data, append = TRUE)
  print("Adding Orders Details Data")
  RSQLite::dbWriteTable(my_connection, "ORDERS_DETAILS", order_details_file, append = TRUE)
  print("Adding Warehouse Data")
  RSQLite::dbWriteTable(my_connection, "WAREHOUSES", warehouse_data, append = TRUE)
  
  # Check referential integrity in each table where foreign keys are available

  # Get the primary key from the db
  # Execute SQL queries to fetch primary keys from each parent table
  sql_query_suppliers <- "SELECT supplier_id FROM SUPPLIERS"
  sql_query_customers <- "SELECT customer_id FROM CUSTOMERS"
  sql_query_products <- "SELECT product_id FROM PRODUCTS"
  sql_query_order_details <- "SELECT order_id FROM ORDERS_DETAILS"
  sql_query_category <- "SELECT category_id FROM CATEGORY"
  sql_query_warehouse <- "SELECT warehouse_id FROM WAREHOUSES"
  
  # Execute SQL queries and fetch results
  suppliers_db <- dbGetQuery(my_connection, sql_query_suppliers)
  customer_db <- dbGetQuery(my_connection, sql_query_customers)
  products_db <- dbGetQuery(my_connection, sql_query_products)
  order_details_db <- dbGetQuery(my_connection, sql_query_order_details)
  category_db <- dbGetQuery(my_connection, sql_query_category)  
  warehouse_db <- dbGetQuery(my_connection, sql_query_warehouse) 
  
  # Function to check referential integrity for foreign keys
  check_referential_integrity <- function(child_df, foreign_key_column, parent_df) {
    foreign_keys <- unique(child_df[[foreign_key_column]])
    parent_keys <- unique(parent_df[[foreign_key_column]])
    missing_keys <- setdiff(foreign_keys, parent_keys)
    if (length(missing_keys) > 0) {
      return(missing_keys)
    } else {
      return(NULL)
    }
  }
 
  # Function to check referential integrity for foreign keys which refer to themselves
  check_referential_integrity2 <- function(child_df, foreign_key_column, parent_df, parent_key) {
    foreign_keys <- unique(child_df[[foreign_key_column]])
    parent_keys <- unique(parent_df[[parent_key]])
    missing_keys <- setdiff(foreign_keys, parent_keys)
    if (length(missing_keys) > 0) {
      return(missing_keys)
    } else {
      return(NULL)
    }
  }
  
  
  # Product table foreign key
  missing_foreign_keys1 <- c()
  missing_foreign_keys1 <- c(missing_foreign_keys1, 
                             check_referential_integrity(product_data, "category_id", category_db))
  if (length(missing_foreign_keys1) > 0) {
    print("Missing foreign keys found:")
    #print(unique(unlist(missing_foreign_keys1)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Products Data")
    RSQLite::dbWriteTable(my_connection, "PRODUCTS", product_data, append = TRUE)
  }
  
  # Advertisements table foreign key
  missing_foreign_keys2 <- c()
  missing_foreign_keys2 <- c(missing_foreign_keys2, 
                             check_referential_integrity(advertisement_data, "product_id", products_db))
  missing_foreign_keys2 <- c(missing_foreign_keys2, 
                             check_referential_integrity(advertisement_data, "supplier_id", suppliers_db))
  if (length(missing_foreign_keys2) > 0) {
    print("Missing foreign keys found:")
    #print(unique(unlist(missing_foreign_keys2)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Ads Data")
    RSQLite::dbWriteTable(my_connection, "ADVERTISEMENTS", advertisement_data, 
                          append = TRUE)
  }  
  

  # Customer table foreign key
  missing_foreign_keys3 <- c()
  missing_foreign_keys3 <- c(missing_foreign_keys3, 
                             check_referential_integrity2(customer_data, "related_id", customer_db,"customer_id"))
  if (length(missing_foreign_keys3) > 0) {
    print("Missing foreign keys found:")
    #print(unique(unlist(missing_foreign_keys3)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Customer Data")
    RSQLite::dbWriteTable(my_connection, "CUSTOMERS", customer_data, append = TRUE)
  }  
  
  
  # Supply table foreign keys
  missing_foreign_keys4 <- c()
  missing_foreign_keys4 <- c(missing_foreign_keys4, 
                             check_referential_integrity(supply_data, "product_id", products_db))
  missing_foreign_keys4 <- c(missing_foreign_keys4, 
                             check_referential_integrity(supply_data, "supplier_id", suppliers_db))
  if (length(missing_foreign_keys4) > 0) {
    print("Missing foreign keys found:")
    print(unique(unlist(missing_foreign_keys4)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Supply Data")
    RSQLite::dbWriteTable(my_connection, "SUPPLY", supply_data, append = TRUE)
  }  
  
  # Store table foreign keys
  missing_foreign_keys5 <- c()
  missing_foreign_keys5 <- c(missing_foreign_keys5, 
                             check_referential_integrity(store_data, "product_id", products_db))
  missing_foreign_keys5 <- c(missing_foreign_keys5, 
                             check_referential_integrity(store_data, "warehouse_id", warehouse_db))
  if (length(missing_foreign_keys5) > 0) {
    print("Missing foreign keys found:")
    #print(unique(unlist(missing_foreign_keys5)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Store Data")
    RSQLite::dbWriteTable(my_connection, "STORE", store_data, append = TRUE)
  } 
  
  # Orders table foreign keys
  missing_foreign_keys6 <- c()
  missing_foreign_keys6 <- c(missing_foreign_keys6, 
                             check_referential_integrity(order_data, "product_id", products_db))
  missing_foreign_keys6 <- c(missing_foreign_keys6, 
                             check_referential_integrity(order_data, "supplier_id", suppliers_db))
  missing_foreign_keys6 <- c(missing_foreign_keys6, 
                             check_referential_integrity(order_data, "order_id", order_details_db))
  missing_foreign_keys6 <- c(missing_foreign_keys6, 
                             check_referential_integrity(order_data, "customer_id", customer_db))
  if (length(missing_foreign_keys6) > 0) {
    print("Missing foreign keys found:")
    #print(unique(unlist(missing_foreign_keys6)))
  } else {
    print("No missing foreign keys found.")
    print("Adding Order Data")
    RSQLite::dbWriteTable(my_connection, "ORDER", order_data, append = TRUE)
  } 


  print("Writing Table is done")
  
  RSQLite::dbDisconnect(my_connection)
}

```

{{< pagebreak >}}

# Part 3: Data Pipeline Generation

## 3.1 - GitHub Repository and Workflow Setup

Objective: We utilised a GitHub repository to manage and version-control our project, ensuring a structured and organized approach to project management.

Approach: Our GitHub repository contains essential files and folders, including:

-   R folder: Contains all R scripts for data processing and analysis.

-   data_upload folder: Houses new data set to be uploaded, facilitating easy access and management of incoming data.

-   data_versions folder: Contains all the datasets in csv file for all the tables for push

-   database folder: Stores the database file, ensuring centralised access to the project's database.

-   qmd file: Serves as the project report, documenting project progress, findings, and insights.

-   figures folder: Stores all plots generated by R scripts, enhancing visualisation and analysis capabilities.

-   project_pics folder: Contains project structure-related images for reference and documentation.

-   github workflow folder: Houses the GitHub Actions workflow, streamlining automation and continuous integration processes.

-   README.md file: Provides essential information about the project, including setup instructions, project overview, and contact details.

## 3.2 - GitHub Actions for Continuous Integration

Objective: To automate data validation, database updates, and basic data analysis tasks using GitHub Actions, ensuring efficient project management and maintenance.

Approach: We implemented a GitHub Actions workflow triggered on specific events, such as push events to the main branch. The workflow consists of the following steps:

1.  Checkout code: Retrieves the project code from the repository for further processing.

2.  Setup R environment: Configures the R environment required for executing R scripts.

3.  Cache R packages: Caches R packages to speed up workflow execution by avoiding redundant package installations.

4.  Install packages: Installs necessary R packages, including readr, RSQLite, stringr, dplyr, fs, and ggplot2, facilitating data processing and analysis.

5.  Execute R script: Runs the specified R script (transformation.R), performing data transformation and analysis tasks.

6.  Add files: Adds generated plot figures to the repository for version control and documentation.

7.  Commit files: Commits changes made to the repository, ensuring changes are tracked and documented.

8.  Push changes: Pushes committed changes to the main branch of the GitHub repository, updating the repository with the latest data and analysis results.

Through effective GitHub repository and workflow setup, coupled with GitHub Actions for continuous integration, we have established a robust framework for managing, version-controlling, and automating project tasks. This streamlined approach enhances collaboration, ensures data quality and integrity, and promotes efficient project maintenance and scalability.

```{r github_workflow, eval=FALSE, warning=FALSE}

on:
  schedule:
    - cron: '0 */5 * * *' # Run every 5 hours
  push:
    branches: [ main ]
    #paths:
    # - 'data_upload/**' # Trigger only when changes occur in the data_upload folder or its subfolders
  
    
jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v2
      - name: Setup R environment
        uses: r-lib/actions/setup-r@v2
        with:
          r-version: '4.3.3'
      - name: Cache R packages
        uses: actions/cache@v2
        with:
          path: ${{ env.R_LIBS_USER }}
          key: ${{ runner.os }}-r-${{ hashFiles('**/lockfile') }}
          restore-keys: |
            ${{ runner.os }}-r-
      - name: Install packages
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          Rscript -e 'install.packages(c("readr","RSQLite","stringr","dplyr","fs","ggplot2","tidyverse","patchwork"))'
      - name: Execute R script
        run: |
          Rscript R/transformation.R
      - name: Add files
        run: |
          git config --local --unset-all "http.https://github.com/.extraheader"
          git config --global user.email "knazish1122@gmail.com"
          git config --global user.name "Nazish-Khan"
          git add --all figures/
      - name: Commit files
        run: |
          git commit -m "Add Plot figure"
      - name: Push changes
        uses: ad-m/github-push-action@v0.6.0
        with:
            github_token: ${{ secrets.MY_TOKEN }}
            branch: main
```

# Part 4: Data Analysis and Visualisation

In this final part, we will analyse and visualise our data sets and perform a fundamental analysis by manipulating data using RSQLite and visualising them using the package "ggplot2"

```{r warning=FALSE}
# Get the required data from the database

library(patchwork)
library(ggplot2)
conn <- RSQLite::dbConnect(RSQLite::SQLite(),dbname = "database/database.db")
# Get data from database into a dataframe
tables_to_read <- c("ADVERTISEMENTS", "WAREHOUSES", "SUPPLY","STORE","PRODUCTS"
                    ,"ORDERS_DETAILS","ORDERS","CUSTOMERS","SUPPLIERS","CATEGORY")

# Initialise an empty list to store data frames
dfs <- list()

# Loop through each table
for (table in tables_to_read) {
  # Read data from the table into a data frame
  query <- paste("SELECT * FROM", table)
  dfs[[table]] <- dbGetQuery(conn, query)
}

product <- dfs$PRODUCTS
order_details <- dfs$ORDERS_DETAILS
category <- dfs$CATEGORY
order <- dfs$ORDERS
customer <- dfs$CUSTOMERS
supplier <- dfs$SUPPLIERS
warehouse <- dfs$WAREHOUSES
supply <- dfs$SUPPLY
store <- dfs$STORE
ads <- dfs$ADVERTISEMENTS

# Write the SQL query to join the tables
sql_query <- "
SELECT *
FROM `ORDERS` o
JOIN PRODUCTS p ON o.product_id = p.product_id
JOIN SUPPLIERS s ON o.supplier_id = s.supplier_id
JOIN ORDERS_DETAILS od ON o.order_id = od.order_id
JOIN CUSTOMERS c ON o.customer_id = c.customer_id
"
sql_supply_query <- "
SELECT *
FROM `SUPPLY` s
JOIN PRODUCTS p ON s.product_id = p.product_id
"

# Execute the SQL query and fetch the result into a dataframe
result_df <- dbGetQuery(conn, sql_query)

supply_product_df <- dbGetQuery(conn, sql_supply_query)


RSQLite::dbDisconnect(conn)

# Define the color palette
color_palette <- c("purple","pink","darkblue","blue",
                   "lightblue","black","darkgrey","white")

# Check for duplicate column names
duplicate_columns <- anyDuplicated(names(result_df)) > 0

# If there are duplicate column names, remove them
if (duplicate_columns) {
  result_df <- result_df[, !duplicated(names(result_df))]
}

# Define the directory to save the figures
figure_directory <- "figures/"

# Create the directory if it doesn't exist
if (!dir.exists(figure_directory)) {
  dir.create(figure_directory)
}

# Save each plot as an image
this_filename_date <- as.character(Sys.Date())
this_filename_time <- as.character(format(Sys.time(), format = "%H_%M"))
```

## 4.1 - Product Performance Analysis

**Top-Bottom Selling Products:**

```{r warning=FALSE}
# Best-selling products / low-selling products and their categories.
 
# Summarize total quantity sold by product
product_sales_grouped <- result_df %>%
  group_by(product_name, category_id) %>%
  summarise(total_quantity_sold = sum(order_quantity), .groups = 'drop')

# Find the top 5 best-selling products
top_products <- product_sales_grouped %>%
  arrange(desc(total_quantity_sold)) %>%
  slice_head(n = 5)

# Find the below 5 low-selling products
low_products <- product_sales_grouped %>%
  arrange(total_quantity_sold) %>%
  slice_head(n = 5)

# Bar plot of best-selling products
top10_product <- ggplot(top_products, aes(x = total_quantity_sold, 
                                          y = reorder(product_name, total_quantity_sold), fill = category_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Top 5 Best-Selling Products",
       x = "Total Quantity Sold",
       y = "Product Name") +
  theme_minimal() +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        panel.background = element_rect(fill = "white"),
        legend.position = "none")

# Bar plot of low-selling products
bottom10_product <- ggplot(low_products, aes(x = total_quantity_sold, 
                                             y = reorder(product_name, desc(total_quantity_sold)), fill = category_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Below 5 Low-Selling Products",
       x = "Total Quantity Sold",
       y = "Product Name") +
  theme_minimal() +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        panel.background = element_rect(fill = "white"),
        legend.position = "none")

# Arrange plots into a single plot
combined_plot <- top10_product + bottom10_product +
  plot_layout(nrow = 2)

# Print the combined plot
print(combined_plot)


```

This bar charts display the top five best-selling products and the bottom five low-selling products. The Reebok Classic Leather is the best-selling product, whereas the Hunger Games Series has the poorest performance.

**Reviews based Order Status Change Analysis:**

```{r warning=FALSE}

# Density plot
density_plot <- ggplot(result_df, aes(x = product_reviewscore, fill = order_status)) +
  geom_density(alpha = 0.8) +
  labs(title = "Density of Product Review Scores by Order Status",
       x = "Product Review Score",
       y = "Density",
       fill = "Order Status") +
  scale_fill_manual(values = color_palette) +
  theme_minimal()


# Save plot figure
ggsave(filename = paste0(figure_directory, "selling_product_trend_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = combined_plot)
ggsave(filename = paste0(figure_directory, "rating_status_plot_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = density_plot)

# Print the plot
print(density_plot)
```

This graph exhibits the relationship between the number of each order status and the product review score; we can notice that almost all Cancelled orders receive a review score lower than 3. Also, the processing order needs a better product review score.

## 4.2 - Product-Category Analysis

**Reviews based Category Analysis:**

```{r warning=FALSE}
# Calculate average review scores for each category
category_review_scores <- product %>%
  group_by(category_id) %>%
  summarise(average_review_score = mean(product_reviewscore, na.rm = TRUE))

# Join with 'CATEGORY' to get category names
category_review_scores <- category_review_scores %>%
  left_join(category, by = "category_id")

# Bar plot of average review scores by category
avg_review_plot <- ggplot(category_review_scores, 
                          aes(x = reorder(category_name, -average_review_score), y = average_review_score, fill = category_id)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(average_review_score, 2)), colour = "white", vjust = -0.5, 
            color = "black", size = 3.5) + 
  labs(title = "Average Customer Values by Category",
       x = "Category",
       y = "Average Review Score") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = color_palette) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), 
        panel.background = element_rect(fill = "white", color = "blue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "blue"),
        axis.title = element_text(color = "blue"),
        axis.text = element_text(color = "blue"),
        legend.position = "none")

print(avg_review_plot)


```

Among all the product categories, the products in these three categories received the most favourable reviews from our customers.

**Profit Margin Analysis:**

```{r warning=FALSE}
# Calculate profit margin
supply_product_df_origin <- supply_product_df # copy for a return data
supply_product_df$profit_margin <- supply_product_df$product_price - 
  supply_product_df$product_cost
 
# Check for duplicate column names
duplicate_columns1 <- anyDuplicated(names(supply_product_df)) > 0
 
# If there are duplicate column names, remove them
if (duplicate_columns1) {
  supply_product_df <- supply_product_df[, !duplicated(names(supply_product_df))]
}
 
# Mutate the Category ID to name
matchindex <- match(supply_product_df$category_id,category_data$category_id)
supply_product_df$category_id <- category_data$category_name[matchindex]
 
# Margin Plot for the product wise profit average
margin_plot <- ggplot(supply_product_df, aes(x = category_id, y = profit_margin)) +
  geom_boxplot() +  # Boxplot to show distribution
  stat_summary(fun.y = mean, geom = "line", aes(group = 1), color = "purple") +
  stat_summary(fun.y = min, geom = "line", aes(group = 1), color = "grey") + 
  stat_summary(fun.y = max, geom = "line", aes(group = 1), color = "blue") + 
  labs(title = "Profit Margin by Product",
       x = "Product Name",
       y = "Profit Margin") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))+ # Rotate x-axis labels
  scale_fill_manual(values = color_palette) +
  theme_minimal()
 
 
# Save plot figure
ggsave(filename = paste0(figure_directory, "review_category_plot_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = avg_review_plot)
ggsave(filename = paste0(figure_directory, "profit_margin_plot_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = margin_plot)
 
## return the data to its original form again ##
supply_product_df <- supply_product_df_origin
 
print(margin_plot)
```

<<<<<<< HEAD
As can be seen from the chart, the gray line displays the lowest margin of each product category. We can observe that each product category's lowest margins are aligned, with no significant difference. In comparison, each category's mean and the highest margin figure seem significant. The electronic section exhibited the best performance among all sections, as can be noticed from the purple line that represents the mean of the margin and the blue line that represents the highest margin.

## 4.3 - Customer Analysis
=======
As can be seen from the chart, the profit margin for the product in an electronic category (CAT5960069590) is the highest among all categories. The green line displays the lowest margin of each product category. We can observe that each product category's lowest margins are aligned with no significant difference. In comparison, each category's mean and the highest margin figure seem significant. The electronic section exhibited the best performance among all sections, as can be noticed from the red line that represents the mean of the margin and the black line that represents the highest margin.
>>>>>>> e00da8c629028b9e55486b9174762a770f5a5eca

**Customer Segmentation:**

```{r warning=FALSE}
# Customer Segmentation

# Segment customers based on country
customers_country <- customer %>%
  group_by(customer_country) %>%
  summarise(total_customers = n())

# Plot for number of customers by country
cust_segm <- ggplot(customers_country, aes(x = customer_country, y = total_customers, 
                                           fill = customer_country)) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Customers by Country",
       x = "Country",
       y = "Number of Customers") +
  #scale_fill_discrete(name = "Country")+
  scale_fill_manual(values = color_palette) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), 
        panel.background = element_rect(fill = "white", color = "blue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "blue"),
        axis.title = element_text(color = "blue"),
        axis.text = element_text(color = "blue"),
        legend.title = element_text(color = "blue"),
        legend.text = element_text(color = "blue"))

print(cust_segm)


```

These bar charts illustrate the distribution of our application users by each country. Most of our users are live or have placed an order in France.

**Customer Preferences in France:**

```{r warning=FALSE}
# Merge category name to the result_df table
matchindex <- match(result_df$category_id,category$category_id)
result_df$category_name <- category$category_name[matchindex]
 
# Select France customers
product_fra <- result_df %>%
  filter(customer_country == "France")
product_fra_cate <- product_fra %>%
  group_by(category_name) %>%
  summarise(quantity=sum(order_quantity))
 
# Calculate the percentage of product category in France
category_totals <- aggregate(quantity ~ category_name, data = product_fra_cate, sum)
total_quantity <- sum(category_totals$quantity)
category_totals$percentage <- category_totals$quantity / total_quantity * 100
 
# Plot the pie chart
cust_france_plot <- ggplot(category_totals, aes(x = "", y = percentage, fill = category_name)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(aes(label = paste(round(percentage, 1), "%")), color = 'white',position = position_stack(vjust = 0.5)) +
  scale_fill_manual(values = color_palette) +
  labs(title = "Customer Preferences in France",
       fill = "Category",
       x = NULL, y = NULL) +
  theme_void() +
  theme(legend.position = "right") +
theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), 
        panel.background = element_rect(fill = "white", color = "blue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "blue"),
        axis.title = element_text(color = "blue"),
        axis.text = element_text(color = "white"),
        legend.title = element_text(color = "blue"),
        legend.text = element_text(color = "blue"))


# Save plot figure
ggsave(filename = paste0(figure_directory, "customer_prefrences_plot-", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = cust_france_plot)
print(cust_france_plot)
```

The above is showcasing the categories including Books, Electronics, and Footwear distribution in France country for total orders.

**Payment Method Trend:**

```{r warning=FALSE}
# Create the payment analysis ggplot
payment_trend_plot <- ggplot(result_df, aes(x = payment_date, fill = payment_method)) +
  geom_bar(stat = "count", position = "dodge") +
  #scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  labs(title = "Payment Method Usage Over Time",
       x = "Payment Date",
       y = "Number of Payments",
       fill = "Payment Method") +
scale_fill_manual(values = color_palette) +
theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1), 
        panel.background = element_rect(fill = "white", color = "blue"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.line = element_line(color = "blue"),
        axis.title = element_text(color = "blue"),
        axis.text = element_text(color = "blue"),
        legend.title = element_text(color = "blue"),
        legend.text = element_text(color = "blue"))




# Save plot figure
ggsave(filename = paste0(figure_directory, "customer_segment_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = cust_segm)
ggsave(filename = paste0(figure_directory, "payment_trend_plot_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = payment_trend_plot)

# Print the plot
print(payment_trend_plot)
```

The bar charts display the distribution of payment methods, the most common method overtime is a Bank transfer.

## 4.4 - Ads vs Supplier Analysis

**Top-Bottom Supplier Contribution in Ads Revenue:**

```{r warning=FALSE}
# Advertisement Analysis

# Convert date format to Date type
ads$ads_startdate <- as.Date(ads$ads_startdate, format = "%d/%m/%y")
ads$ads_enddate <- as.Date(ads$ads_enddate, format = "%d/%m/%y")

# Calculate duration in days
ads$duration <- as.numeric(difftime(ads$ads_enddate, ads$ads_startdate, units = "days"))


ads$price_per_day <- round(ads$ads_price / ads$duration,2)

# Find the top 5 suppliers paying the highest price per day
top_suppliers <- ads %>%
  arrange(desc(price_per_day)) %>%
  slice_head(n = 5) %>%
  group_by(supplier_id) %>%
  summarise(avg_price_per_day = mean(price_per_day), .groups = 'drop')

# Find the bottom 5 suppliers paying the lowest price per day
bottom_suppliers <- ads %>%
  arrange(price_per_day) %>%
  slice_head(n = 5) %>%
  group_by(supplier_id) %>%
  summarise(avg_price_per_day = mean(price_per_day), .groups = 'drop')


# Bar plot of top 5 suppliers paying the highest price per day
top5_suppliers_plot <- ggplot(top_suppliers, aes(x = avg_price_per_day, 
                                                 y = reorder(supplier_id, avg_price_per_day), fill = supplier_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Top 5 Suppliers with Highest Price per Day",
       x = "Average Price per Day",
       y = "Supplier ID") +
  scale_fill_manual(values = color_palette) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        panel.background = element_rect(fill = "white"),
        legend.position = "none")

# Bar plot of bottom 5 suppliers paying the lowest price per day
bottom5_suppliers_plot <- ggplot(bottom_suppliers, aes(x = avg_price_per_day, 
                                                       y = reorder(supplier_id, avg_price_per_day), fill = supplier_id)) +
  geom_bar(stat = "identity") +
  labs(title = "Bottom 5 Suppliers with Lowest Price per Day",
       x = "Average Price per Day",
       y = "Supplier ID") +
  theme_minimal() +
  scale_fill_manual(values = color_palette) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1),
        panel.background = element_rect(fill = "white"),
        legend.position = "none")

# Arrange plots into a single plot
combined_suppliers_plot <- top5_suppliers_plot + bottom5_suppliers_plot +
  plot_layout(nrow = 2)


# Save plot figure

ggsave(filename = paste0(figure_directory, "ads_supplier_plot_", 
                         this_filename_date, "_", this_filename_time, ".png"), width = 6, height = 5, plot = combined_suppliers_plot)

# Print the combined plot
print(combined_suppliers_plot)
```

<<<<<<< HEAD
This bar chart displays the top 5 suppliers who purchase our advertisement at the most expensive rate and the bottom 5 of them. We find the Stewart Group (SUP261309764) is the customer who is the most willing to pay for our advertisement service.

# Conclusion

In conclusion, to create our model of an e-commerce platform, we utilised a 7-entity database structure. The ER diagram visually represented each entity and relationship, essential for understanding data organisation whilst taking into account certain assumptions. The data was then normalised to 3NF and physical schema creation was outlined to using SQL (DDL) to ensure database integrity.

Subsequently, by leveraging A.I., we generated datasets for each entity. To ensure synthetic data was realistic and logical quality assurance procedures and data integrity checks (such as Referential Integrity and Entity Integrity Checks)

A GitHub repository and a workflow process was then utilised to manage and version-control our project and automate data management process by establishing validation process for every 2 hours. Efficient project management framework through continuous integration with GitHub Actions. Lastly data Analysis and visualisations were carried out to identify data trends and statistical insights.
=======
These bar charts display the top 5 suppliers who purchase our advertisement at the most expensive rate and the bottom 5 of them. We find the Stewart Group (SUP261309764) is the customer who is the most willing to pay for our advertisement service.
>>>>>>> e00da8c629028b9e55486b9174762a770f5a5eca
